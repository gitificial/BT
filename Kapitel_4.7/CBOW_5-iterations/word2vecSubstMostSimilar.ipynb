{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from os.path import join, exists, split\n",
    "import os\n",
    "\n",
    "import sys\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Use functions from data_helpers.py to get training and test instances.\n",
    "\"\"\"\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Load data and labels from files\n",
    "    \"\"\"\n",
    "\n",
    "#-------------------- Generate TRAIN subst --------------------------------------\n",
    "    positive_examples_lowerhalf_nounsSubst = list(open(\"./data/positive_lowerhalf_shuffled_100.txt\").readlines())\n",
    "    positive_examples_lowerhalf_nounsSubst = [s.strip() for s in positive_examples_lowerhalf_nounsSubst]\n",
    "\n",
    "    negative_examples_lowerhalf_nounsSubst = list(open(\"./data/negative_lowerhalf_shuffled_100.txt\").readlines())\n",
    "    negative_examples_lowerhalf_nounsSubst = [s.strip() for s in negative_examples_lowerhalf_nounsSubst]\n",
    "\n",
    "    # Split by words\n",
    "    x_text_subst = positive_examples_lowerhalf_nounsSubst + negative_examples_lowerhalf_nounsSubst\n",
    "    x_text_subst = [clean_str(sent) for sent in x_text_subst]\n",
    "    x_text_subst = [s.split(\" \") for s in x_text_subst]\n",
    "\n",
    "    # Generate labels\n",
    "    positive_labels_subst = [[0, 1] for _ in positive_examples_lowerhalf_nounsSubst]\n",
    "    negative_labels_subst = [[1, 0] for _ in negative_examples_lowerhalf_nounsSubst]\n",
    "    y_subst = np.concatenate([positive_labels_subst, negative_labels_subst], 0)\n",
    "\n",
    "\n",
    "#-------------------- Generate TEST  --------------------------------------\n",
    "    positive_examples_upperHalf = list(open(\"./data/positive_upperhalf_shuffled_800.txt\").readlines())\n",
    "    positive_examples_upperHalf = [s.strip() for s in positive_examples_upperHalf]\n",
    "\n",
    "    negative_examples_upperHalf = list(open(\"./data/negative_upperhalf_shuffled_800.txt\").readlines())\n",
    "    negative_examples_upperHalf = [s.strip() for s in negative_examples_upperHalf]\n",
    "\n",
    "    # Split by words\n",
    "    x_text_upper = positive_examples_upperHalf + negative_examples_upperHalf\n",
    "    x_text_upper = [clean_str(sent) for sent in x_text_upper]\n",
    "    x_text_upper = [s.split(\" \") for s in x_text_upper]\n",
    "\n",
    "    # Generate labels\n",
    "    positive_labels_upper = [[0, 1] for _ in positive_examples_upperHalf]\n",
    "    negative_labels_upper = [[1, 0] for _ in negative_examples_upperHalf]\n",
    "    y_upper = np.concatenate([positive_labels_upper, negative_labels_upper], 0)\n",
    "\n",
    "    return [x_text_subst, y_subst, x_text_upper, y_upper]\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, max_length, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length with the string <PAD/>. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    sequence_length = max_length\n",
    "    print(\"Print sequence_length: \", sequence_length)\n",
    "\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess data.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences_train, labels_train, sentences_test, labels_test = load_data_and_labels()\n",
    "\n",
    "    sentences_all = np.concatenate([sentences_train, sentences_test], 0)\n",
    "\n",
    "    max_length = max(len(x) for x in sentences_all)\n",
    "    sentences_all_padded = pad_sentences(sentences_all, max_length)\n",
    "\n",
    "    max_length = max(len(x) for x in sentences_all)\n",
    "\n",
    "    sentences_train_padded = pad_sentences(sentences_train, max_length)\n",
    "    sentences_test_padded = pad_sentences(sentences_test, max_length)\n",
    "\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_all_padded)\n",
    "\n",
    "    x_train, y_train = build_input_data(sentences_train_padded, labels_train, vocabulary)\n",
    "    x_test, y_test = build_input_data(sentences_test_padded, labels_test, vocabulary)\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test, vocabulary, vocabulary_inv]\n",
    "\n",
    "def cnn_load_data():\n",
    "    \"\"\"\n",
    "    Creates the training instances.\n",
    "    Returns the instances and the inverted vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    x_train, y_train, x_test, y_test, vocabulary, vocabulary_inv_list = load_data()\n",
    "    vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "    \n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    # Lable: positiv [0, 1] -> index 1, negativ [1, 0] -> index 0\n",
    "    y_train = y_train.argmax(axis=1)\n",
    "    y_test = y_test.argmax(axis=1)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print sequence_length:  374\n",
      "Print sequence_length:  374\n",
      "Print sequence_length:  374\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create training and test instances.\n",
    "\"\"\"\n",
    "x_train, y_train, x_test, y_test, vocabulary_inv = cnn_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function from w2v.py to create and train word2vec model\n",
    "\"\"\"\n",
    "\n",
    "def train_word2vec_model(sentence_matrix, vocabulary_inv, skipgram, iterations, num_features=300, min_word_count=1, context=10):\n",
    "    \"\"\"\n",
    "    Trains, saves, loads Word2Vec model\n",
    "    Returns initial weights for embedding layer.\n",
    "   \n",
    "    inputs:\n",
    "    sentence_matrix # int matrix: num_sentences x max_sentence_len\n",
    "    vocabulary_inv  # dict {int: str}\n",
    "    num_features    # Word vector dimensionality                      \n",
    "    min_word_count  # Minimum word count                        \n",
    "    context         # Context window size \n",
    "    \"\"\"\n",
    "    \n",
    "    model_dir = 'models'\n",
    "    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n",
    "    model_name = join(model_dir, model_name)\n",
    "    \n",
    "    # Set values for various parameters\n",
    "    num_workers = 2  # Number of threads to run in parallel\n",
    "    downsampling = 1e-3  # Downsample setting for frequent words\n",
    "\n",
    "    print('Training Word2Vec model...')\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n",
    "\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                            size=num_features, min_count=min_word_count,\n",
    "                                            window=context, sample=downsampling, sg = skipgram, iter=iterations)\n",
    "\n",
    "    # If we don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    embedding_model.init_sims(replace=True)\n",
    "\n",
    "    # Saving the model for later use. You can load it later using Word2Vec.load()\n",
    "    if not exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "    embedding_model.save(model_name)\n",
    "\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Saving Word2Vec model '50features_1minwords_10context'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the word2vec model with 200 Training instances (100 pos./100 neg.) + 1600 Test instances (800 pos./800 neg.)\n",
    "\n",
    "Use CBOW algorithm, train for 5 Iterations.\n",
    "\"\"\"\n",
    "# 0: CBOW, 1: skipgram\n",
    "skipgram = 0\n",
    "\n",
    "# number of training iterations\n",
    "iterations = 5\n",
    "\n",
    "embedding_dim = 50\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "model = train_word2vec_model(np.vstack((x_train, x_test)), vocabulary_inv, skipgram, iterations, num_features=embedding_dim, min_word_count=min_word_count, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.most_similar('bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.most_similar('bed')[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Open positive lowerhalf shuffeld 100 training instances file. \n",
    "    Read file line by line, find Nouns ('NN', 'NNS', 'NNP', 'NNPS') \n",
    "    and replace with most_similar word, selected rank depending on actual position.\n",
    "    If no most_similar exists, leave the actual word. \n",
    "    Write un- and modified lines to new file.\n",
    "    Create one file per most_similar rank position.\n",
    "\"\"\"\n",
    "\n",
    "new_line = []\n",
    "string = ''\n",
    "i = 0\n",
    "linenr = 0\n",
    "\n",
    "for position in range(0, 39, 1):\n",
    "    with open(\"./data/positive_lowerhalf_shuffled_100.txt\") as infile:\n",
    "        for line in infile:\n",
    "            new_line = []\n",
    "            content = TextBlob(line)\n",
    "            nouns = [n for n,t in content.tags if(t == 'NN' or t == 'NNS' or t == 'NNP' or t == 'NNPS')]\n",
    "            for word, tag in content.tags:\n",
    "                if(tag == 'NN' or tag == 'NNS' or tag == 'NNP' or tag == 'NNPS'):\n",
    "                    if word in model.wv.vocab:\n",
    "                        new_line.append(str(model.wv.most_similar(word, topn=50)[position][0]))\n",
    "                    else:\n",
    "                        new_line.append(word)\n",
    "                else:\n",
    "                    new_line.append(word)\n",
    "\n",
    "            if(linenr % 10000 == 0):\n",
    "                print(linenr)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            linenr = linenr + 1\n",
    "            \n",
    "            filename = './data/positive_lowerhalf_shuffled_exchange_' + str(position) + '.txt'\n",
    "            with open(filename, 'a') as file:\n",
    "                if(len(new_line) > 0):\n",
    "                    file.write(' '.join(new_line) + '\\n')\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Open negative lowerhalf shuffeld 100 training instances file. \n",
    "    Read file line by line, find Nouns ('NN', 'NNS', 'NNP', 'NNPS') \n",
    "    and replace with most_similar word, selected rank depending on actual position.\n",
    "    If no most_similar exists, leave the actual word. \n",
    "    Write un- and modified lines to new file.\n",
    "    Create one file per most_similar rank position.\n",
    "\"\"\"\n",
    "\n",
    "new_line = []\n",
    "string = ''\n",
    "i = 0\n",
    "linenr = 0\n",
    "\n",
    "for position in range(0, 39, 1):\n",
    "    with open(\"./data/negative_lowerhalf_shuffled_100.txt\") as infile:\n",
    "        for line in infile:\n",
    "            new_line = []\n",
    "            content = TextBlob(line)\n",
    "            nouns = [n for n,t in content.tags if(t == 'NN' or t == 'NNS' or t == 'NNP' or t == 'NNPS')]\n",
    "            for word, tag in content.tags:\n",
    "                if(tag == 'NN' or tag == 'NNS' or tag == 'NNP' or tag == 'NNPS'):\n",
    "                    if word in model.wv.vocab:\n",
    "                        new_line.append(str(model.wv.most_similar(word, topn=50)[position][0]))\n",
    "                    else:\n",
    "                        new_line.append(word)\n",
    "                else:\n",
    "                    new_line.append(word)\n",
    "\n",
    "            if(linenr % 10000 == 0):\n",
    "                print(linenr)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            linenr = linenr + 1\n",
    "\n",
    "            filename = './data/negative_lowerhalf_shuffled_exchange_' + str(position) + '.txt'\n",
    "            with open(filename, 'a') as file:\n",
    "                if(len(new_line) > 0):\n",
    "                    file.write(' '.join(new_line) + '\\n')\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
